{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QxDXip77y_v"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.argv = [sys.argv[0]]  # ignore Colab's extra arguments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code cell is importing the sys module, which provides access to system-specific parameters and functions. The line sys.argv = [sys.argv[0]] is resetting the sys.argv list to contain only the name of the script being executed. This is often done in environments like Google Colab to remove command-line arguments that are automatically added by the environment, which can interfere with scripts that expect a specific number of arguments."
      ],
      "metadata": {
        "id": "IDXj0nGZ738t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "scrape_livingcost_india_inr_hardening.py\n",
        "- Usage:\n",
        "    python scrape_livingcost_india_inr_hardening.py           # full run\n",
        "    python scrape_livingcost_india_inr_hardening.py --test 10 # run only first 10 page candidates\n",
        "- Requirements:\n",
        "    pip install requests beautifulsoup4 pandas openpyxl\n",
        "- Output:\n",
        "    - livingcost_india_all_inr.csv\n",
        "    - debug_logs.txt\n",
        "    - optional ./html_snippets/<city_slug>.html (for debugging)\n",
        "\"\"\"\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import time, re, csv, os, argparse, sys, traceback\n",
        "import pandas as pd\n",
        "\n",
        "BASE = \"https://livingcost.org\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117 Safari/537.36\"}\n",
        "OUT_CSV = \"livingcost_india_all_inr.csv\"\n",
        "LOGFILE = \"debug_logs.txt\"\n",
        "SNIPPET_DIR = \"html_snippets\"\n",
        "\n",
        "# fallback rate (used if exchange API fails)\n",
        "FALLBACK_USD_TO_INR = 87.8638\n",
        "\n",
        "# small helper logging\n",
        "def log(msg):\n",
        "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"[{ts}] {msg}\")\n",
        "    with open(LOGFILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"[{ts}] {msg}\\n\")\n",
        "\n",
        "def get(url, timeout=15):\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "            if r.status_code == 200:\n",
        "                return r\n",
        "            log(f\"Non-200 for {url}: {r.status_code}\")\n",
        "        except Exception as e:\n",
        "            log(f\"Request error for {url}: {e} (attempt {attempt+1})\")\n",
        "        time.sleep(1 + attempt)\n",
        "    return None\n",
        "\n",
        "def fetch_usd_to_inr():\n",
        "    try:\n",
        "        r = requests.get(\"https://api.exchangerate.host/latest?base=USD&symbols=INR\", timeout=8)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        rate = data.get(\"rates\", {}).get(\"INR\")\n",
        "        if rate and rate > 0:\n",
        "            log(f\"Fetched live USD->INR rate: {rate}\")\n",
        "            return float(rate)\n",
        "    except Exception as e:\n",
        "        log(f\"Could not fetch live rate: {e}\")\n",
        "    log(f\"Using fallback USD->INR rate: {FALLBACK_USD_TO_INR}\")\n",
        "    return FALLBACK_USD_TO_INR\n",
        "\n",
        "def find_india_city_links(limit=None):\n",
        "    # Attempt multiple places that might list India city links\n",
        "    candidates = [\n",
        "        \"/cost/india\",\n",
        "        \"/cost/country/india\",   # sometimes sites use different index paths\n",
        "        \"/cost/asia/india\"\n",
        "    ]\n",
        "    links = set()\n",
        "    for p in candidates:\n",
        "        idx_url = urljoin(BASE, p)\n",
        "        r = get(idx_url)\n",
        "        if not r:\n",
        "            log(f\"Index page not reachable: {idx_url}\")\n",
        "            continue\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        # 1) common anchor pattern\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            href = a[\"href\"].strip()\n",
        "            if \"/cost/india/\" in href and len(href.split(\"/\")) >= 4:\n",
        "                links.add(urljoin(BASE, href.split(\"#\")[0]))\n",
        "        # 2) try data attributes or JSON-LD (rare)\n",
        "        # 3) also look for sitemap link on the page\n",
        "        # quick stop if we have many links\n",
        "        if limit and len(links) >= limit:\n",
        "            break\n",
        "        time.sleep(0.2)\n",
        "    links = sorted(links)\n",
        "    log(f\"Found {len(links)} candidate links (sample 10): {links[:10]}\")\n",
        "    return links[:limit] if limit else links\n",
        "\n",
        "# robust numeric extractor: picks first $-prefixed or bare number in given piece of text\n",
        "def extract_first_number(text):\n",
        "    if not text:\n",
        "        return None\n",
        "    # remove non-ascii noise\n",
        "    txt = text.replace(\"\\xa0\", \" \").replace(\",\", \"\")\n",
        "    # try $ followed by number\n",
        "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", txt)\n",
        "    if m:\n",
        "        return float(m.group(1))\n",
        "    # otherwise find any standalone number\n",
        "    m2 = re.search(r\"([0-9]+(?:\\.[0-9]+)?)\", txt)\n",
        "    if m2:\n",
        "        return float(m2.group(1))\n",
        "    return None\n",
        "\n",
        "def parse_city_page(url, save_snippet=False):\n",
        "    r = get(url)\n",
        "    if not r:\n",
        "        log(f\"Failed to fetch page {url}\")\n",
        "        return None\n",
        "    html = r.text\n",
        "    if save_snippet:\n",
        "        os.makedirs(SNIPPET_DIR, exist_ok=True)\n",
        "        slug = urlparse(url).path.strip(\"/\").replace(\"/\", \"_\")\n",
        "        try:\n",
        "            with open(os.path.join(SNIPPET_DIR, f\"{slug}.html\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(html)\n",
        "        except Exception as e:\n",
        "            log(f\"Could not save snippet for {url}: {e}\")\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    # city name\n",
        "    h1 = soup.find(\"h1\")\n",
        "    city = h1.get_text(strip=True) if h1 else url.split(\"/\")[-1]\n",
        "\n",
        "    text = soup.get_text(separator=\" \", strip=True)\n",
        "    # heuristics: look for phrases that site uses\n",
        "    # cost of living one person\n",
        "    cost_patterns = [\n",
        "        r\"Cost of living(?: for)?(?: one person| - one person| \\(one person\\))?:?\\s*\\$?([0-9,]+(?:\\.[0-9]+)?)\",\n",
        "        r\"Cost of living(?:.*?)(?:\\$\\s*([0-9,]+(?:\\.[0-9]+)?))\"\n",
        "    ]\n",
        "    rent_patterns = [\n",
        "        r\"Rent & Utilities(?:.*?)(?:\\$\\s*([0-9,]+(?:\\.[0-9]+)?))\",\n",
        "        r\"Rent(?:.*?)(?:\\$\\s*([0-9,]+(?:\\.[0-9]+)?))\"\n",
        "    ]\n",
        "    salary_patterns = [\n",
        "        r\"Monthly salary after tax(?:.*?)(?:\\$\\s*([0-9,]+(?:\\.[0-9]+)?))\",\n",
        "        r\"Monthly salary(?:.*?)(?:\\$\\s*([0-9,]+(?:\\.[0-9]+)?))\"\n",
        "    ]\n",
        "    def find_first(patterns):\n",
        "        for p in patterns:\n",
        "            m = re.search(p, text, re.I|re.S)\n",
        "            if m:\n",
        "                try:\n",
        "                    val = float(m.group(1).replace(\",\",\"\"))\n",
        "                    return val\n",
        "                except:\n",
        "                    pass\n",
        "        return None\n",
        "\n",
        "    cost = find_first(cost_patterns)\n",
        "    rent = find_first(rent_patterns)\n",
        "    salary = find_first(salary_patterns)\n",
        "\n",
        "    # fallback: look near labels in DOM. Search for label text nodes then siblings\n",
        "    if cost is None:\n",
        "        label_nodes = soup.find_all(text=re.compile(r\"cost of living\", re.I))\n",
        "        for node in label_nodes:\n",
        "            parent = node.parent\n",
        "            snippet = parent.get_text(\" \", strip=True)\n",
        "            cost = extract_first_number(snippet)\n",
        "            if cost:\n",
        "                break\n",
        "    if rent is None:\n",
        "        label_nodes = soup.find_all(text=re.compile(r\"rent & utilities|rent\", re.I))\n",
        "        for node in label_nodes:\n",
        "            parent = node.parent\n",
        "            snippet = parent.get_text(\" \", strip=True)\n",
        "            rent = extract_first_number(snippet)\n",
        "            if rent:\n",
        "                break\n",
        "    if salary is None:\n",
        "        label_nodes = soup.find_all(text=re.compile(r\"monthly salary\", re.I))\n",
        "        for node in label_nodes:\n",
        "            parent = node.parent\n",
        "            snippet = parent.get_text(\" \", strip=True)\n",
        "            salary = extract_first_number(snippet)\n",
        "            if salary:\n",
        "                break\n",
        "\n",
        "    # months covered sentence\n",
        "    months = None\n",
        "    mmon = re.search(r\"enough to cover.*?([0-9]*\\.?[0-9]+)\\s*months?\", text, re.I)\n",
        "    if mmon:\n",
        "        months = float(mmon.group(1))\n",
        "\n",
        "    # If none of primary numbers found, bail out (no data)\n",
        "    if not any([cost, rent, salary, months]):\n",
        "        log(f\"No numeric fields found for {url} ({city}); skipping.\")\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        \"city\": city,\n",
        "        \"cost_one_person_usd\": cost,\n",
        "        \"rent_one_person_usd\": rent,\n",
        "        \"monthly_salary_after_tax_usd\": salary,\n",
        "        \"months_covered\": months,\n",
        "        \"source_url\": url\n",
        "    }\n",
        "\n",
        "def main(limit=None, test_mode=False, save_snippets=False):\n",
        "    open(LOGFILE, \"w\").close()\n",
        "    log(\"START SCRAPE run\")\n",
        "    links = find_india_city_links(limit=limit or (10 if test_mode else None))\n",
        "    if not links:\n",
        "        log(\"No city links found — possible reasons: site structure changed or index page different. See debug_logs.txt.\")\n",
        "        return\n",
        "\n",
        "    rows = []\n",
        "    for i, url in enumerate(links, 1):\n",
        "        log(f\"[{i}/{len(links)}] Processing: {url}\")\n",
        "        try:\n",
        "            parsed = parse_city_page(url, save_snippet=save_snippets)\n",
        "            if parsed:\n",
        "                # compute income after rent (if possible)\n",
        "                rent = parsed.get(\"rent_one_person_usd\") or 0.0\n",
        "                salary = parsed.get(\"monthly_salary_after_tax_usd\") or 0.0\n",
        "                parsed[\"income_after_rent_usd\"] = None if parsed.get(\"monthly_salary_after_tax_usd\") is None else (salary - rent)\n",
        "                rows.append(parsed)\n",
        "                log(f\"Parsed {parsed['city']}: cost={parsed.get('cost_one_person_usd')}, rent={rent}, salary={salary}, months={parsed.get('months_covered')}\")\n",
        "            else:\n",
        "                log(f\"Skipped (no relevant numeric data): {url}\")\n",
        "        except Exception as e:\n",
        "            log(f\"Exception parsing {url}: {e}\")\n",
        "            traceback.print_exc()\n",
        "        time.sleep(0.8)  # politeness\n",
        "\n",
        "    if not rows:\n",
        "        log(\"No rows parsed. Exiting. Check debug_logs.txt and consider running with --test to get snippets.\")\n",
        "        return\n",
        "\n",
        "    rate = fetch_usd_to_inr()\n",
        "    # Build final rows and write CSV\n",
        "    fieldnames = [\"city\",\"cost_one_person_usd\",\"rent_one_person_usd\",\"monthly_salary_after_tax_usd\",\"income_after_rent_usd\",\"months_covered\",\"cost_one_person_inr\",\"rent_one_person_inr\",\"monthly_salary_after_tax_inr\",\"income_after_rent_inr\",\"usd_to_inr_rate_used\",\"source_url\"]\n",
        "    with open(OUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for r in rows:\n",
        "            cost = r.get(\"cost_one_person_usd\")\n",
        "            rent = r.get(\"rent_one_person_usd\") or 0.0\n",
        "            salary = r.get(\"monthly_salary_after_tax_usd\") or 0.0\n",
        "            inc = r.get(\"income_after_rent_usd\")\n",
        "            out = {\n",
        "                \"city\": r.get(\"city\"),\n",
        "                \"cost_one_person_usd\": cost,\n",
        "                \"rent_one_person_usd\": rent if rent != 0.0 else None,\n",
        "                \"monthly_salary_after_tax_usd\": salary if salary != 0.0 else None,\n",
        "                \"income_after_rent_usd\": inc,\n",
        "                \"months_covered\": r.get(\"months_covered\"),\n",
        "                \"cost_one_person_inr\": round(cost * rate, 2) if cost else None,\n",
        "                \"rent_one_person_inr\": round(rent * rate, 2) if rent else None,\n",
        "                \"monthly_salary_after_tax_inr\": round(salary * rate, 2) if salary else None,\n",
        "                \"income_after_rent_inr\": round(inc * rate, 2) if inc is not None else None,\n",
        "                \"usd_to_inr_rate_used\": rate,\n",
        "                \"source_url\": r.get(\"source_url\")\n",
        "            }\n",
        "            writer.writerow(out)\n",
        "    log(f\"Wrote {OUT_CSV} with {len(rows)} rows. Done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--limit\", type=int, default=None, help=\"Limit number of candidate links to scan (for testing)\")\n",
        "    ap.add_argument(\"--test\", action=\"store_true\", help=\"Test mode: only check a small set of candidates\")\n",
        "    ap.add_argument(\"--save-snippets\", action=\"store_true\", help=\"Save HTML snippets for debugging\")\n",
        "    args = ap.parse_args()\n",
        "    main(limit=args.limit, test_mode=args.test, save_snippets=args.save_snippets)\n"
      ],
      "metadata": {
        "id": "GNoUpkf-8CdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The internet = a giant library. Each webpage = a book.\n",
        "\n",
        "The script = a helpful robot librarian that visits pages and copies specific facts into a spreadsheet.\n",
        "\n",
        "How the robot works, in 6 tiny steps:\n",
        "\n",
        "Gets its tools — imports requests (to fetch pages), BeautifulSoup (to read messy HTML), pandas (to make a spreadsheet), and sets some settings (like the site address and polite headers).\n",
        "\n",
        "Knocks and retries — when it asks for a page, it tries a few times if the page doesn’t respond, and logs what happened.\n",
        "\n",
        "Finds city pages — looks at index/catalog pages and grabs all links that look like /cost/india/<city>.\n",
        "\n",
        "Reads each city page — opens the page, uses BeautifulSoup to turn messy HTML into something readable, finds the city name and the numbers for cost/rent/salary by searching for clues and nearby numbers.\n",
        "\n",
        "Converts money — asks a currency service for USD→INR rate (or uses yesterday’s saved rate if needed) and converts dollar amounts to rupees.\n",
        "\n",
        "Saves everything — calculates “income after rent,” collects all city records, and writes them into a CSV spreadsheet."
      ],
      "metadata": {
        "id": "EoYXWcVA8crh"
      }
    }
  ]
}